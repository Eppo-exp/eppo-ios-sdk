import XCTest
@testable import EppoFlagging
import Foundation

class LargeFlagPerformanceTests: XCTestCase {

    func testJSONVsProtobufLazyPerformanceBenchmark() throws {
        // JSON vs Protobuf Lazy performance comparison
        // Focus: Startup performance with same evaluation logic

        print("üöÄ Starting JSON vs Protobuf Lazy Performance Benchmark")
        print("üéØ Focus: Fast startup comparison with correctness validation")

        // Store only the aggregated results, not the full data
        let jsonResults: PerformanceResults = try autoreleasepool {
            print("\nüì¶ Benchmarking JSON Mode...")
            let jsonData = try loadTestDataFile("flags-v1.json")
            print("   üìÑ JSON: \(ByteCountFormatter.string(fromByteCount: Int64(jsonData.count), countStyle: .binary))")

            let results = try benchmarkJSONModeEfficient(jsonData: jsonData)

            print("   üßπ JSON test completed, releasing memory...")
            return results
        } // JSON data automatically released here

        // Force memory cleanup between modes
        autoreleasepool {}

        let protobufResults: PerformanceResults = try autoreleasepool {
            print("\nüì¶ Benchmarking Protobuf Lazy Mode...")
            let protobufData = try loadTestDataFile("flags-v1.pb")
            print("   üß†‚ö° Protobuf: \(ByteCountFormatter.string(fromByteCount: Int64(protobufData.count), countStyle: .binary))")

            let results = try benchmarkProtobufLazyModeEfficient(protobufData: protobufData)

            print("   üßπ Protobuf test completed, releasing memory...")
            return results
        } // Protobuf data automatically released here

        // RESULTS COMPARISON (only store aggregations)
        let startupSpeedup = jsonResults.startupTime / protobufResults.startupTime
        let evaluationSpeedup = protobufResults.evaluationsPerSecond / jsonResults.evaluationsPerSecond

        print("\nüèÜ PERFORMANCE RESULTS:")
        print("üìä JSON: \(Int(jsonResults.startupTime))ms startup, \(Int(jsonResults.evaluationsPerSecond)) evals/sec")
        print("üìä Protobuf: \(Int(protobufResults.startupTime))ms startup, \(Int(protobufResults.evaluationsPerSecond)) evals/sec")
        print("\nüèÅ SPEEDUP:")
        print("   ‚ö° Startup: \(String(format: "%.1f", startupSpeedup))x faster with Protobuf")
        print("   üöÄ Evaluation: \(String(format: "%.1f", evaluationSpeedup))x relative performance")

        // Simple assertions
        XCTAssertLessThan(jsonResults.startupTime, 5000, "JSON startup should be under 5 seconds")
        XCTAssertGreaterThan(protobufResults.evaluationsPerSecond, 50, "Protobuf should handle at least 50 evals/sec")
        XCTAssertGreaterThan(startupSpeedup, 1.0, "Protobuf should have faster startup")

        print("\n‚úÖ JSON vs Protobuf Lazy performance benchmark completed!")
    }

    func testJSONPerformanceBenchmark() throws {
        // JSON vs Protobuf Lazy performance comparison as requested
        print("üöÄ Starting JSON vs Protobuf Lazy Performance Benchmark")
        print("üéØ Focus: Fast startup comparison with correct evaluation logic")

        // Load JSON data
        let jsonData = try loadTestDataFile("flags-v1.json")
        print("   üìÑ JSON: \(ByteCountFormatter.string(fromByteCount: Int64(jsonData.count), countStyle: .binary))")

        // Load protobuf data
        let protobufData = try loadTestDataFile("flags-v1.pb")
        print("   üß†‚ö° Protobuf: \(ByteCountFormatter.string(fromByteCount: Int64(protobufData.count), countStyle: .binary))")

        // JSON benchmark
        print("\nüì¶ Benchmarking JSON Mode...")
        let jsonStart = CFAbsoluteTimeGetCurrent()
        var configuration: Configuration? = try Configuration(flagsConfigurationJson: jsonData, obfuscated: false)
        let jsonStartupTime = (CFAbsoluteTimeGetCurrent() - jsonStart) * 1000

        var jsonClient: EppoClient? = EppoClient.initializeOffline(
            sdkKey: "json-test",
            assignmentLogger: nil,
            initialConfiguration: configuration!
        )
        print("   ‚ö° JSON startup: \(Int(jsonStartupTime))ms")

        // Protobuf benchmark
        print("\nüì¶ Benchmarking Protobuf Lazy Mode...")
        let protobufStart = CFAbsoluteTimeGetCurrent()
        var protobufClient: ProtobufLazyClient? = try ProtobufLazyClient(
            sdkKey: "protobuf-test",
            protobufData: protobufData,
            obfuscated: false,
            assignmentLogger: nil
        )
        let protobufStartupTime = (CFAbsoluteTimeGetCurrent() - protobufStart) * 1000
        print("   ‚ö° Protobuf startup: \(Int(protobufStartupTime))ms")

        // Quick evaluation test (5 flags, 3 subjects each = 15 evaluations)
        let flagKeys = Array(configuration!.flagsConfiguration.flags.keys.prefix(5))

        // JSON evaluation test
        let jsonEvalStart = CFAbsoluteTimeGetCurrent()
        for flagKey in flagKeys {
            if let flag = configuration!.flagsConfiguration.flags[flagKey] {
                for i in 0..<3 {
                    switch flag.variationType {
                    case .boolean:
                        _ = jsonClient!.getBooleanAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: false)
                    case .string:
                        _ = jsonClient!.getStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "")
                    case .integer:
                        _ = jsonClient!.getIntegerAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0)
                    case .numeric:
                        _ = jsonClient!.getNumericAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0.0)
                    case .json:
                        _ = jsonClient!.getJSONStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "{}")
                    }
                }
            }
        }
        let jsonEvalTime = (CFAbsoluteTimeGetCurrent() - jsonEvalStart) * 1000

        // Protobuf evaluation test
        let protobufEvalStart = CFAbsoluteTimeGetCurrent()
        for flagKey in flagKeys {
            if let flagType = protobufClient!.getFlagVariationType(flagKey: flagKey) {
                for i in 0..<3 {
                    switch flagType {
                    case .boolean:
                        _ = protobufClient!.getBooleanAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: false)
                    case .string:
                        _ = protobufClient!.getStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "")
                    case .integer:
                        _ = protobufClient!.getIntegerAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0)
                    case .numeric:
                        _ = protobufClient!.getNumericAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0.0)
                    case .json:
                        _ = protobufClient!.getJSONStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "{}")
                    }
                }
            }
        }
        let protobufEvalTime = (CFAbsoluteTimeGetCurrent() - protobufEvalStart) * 1000

        // Results
        let startupSpeedup = jsonStartupTime / protobufStartupTime
        let evaluationsPerSecondJSON = 15000.0 / jsonEvalTime  // 15 evals * 1000ms
        let evaluationsPerSecondProtobuf = 15000.0 / protobufEvalTime

        print("\nüèÜ PERFORMANCE RESULTS:")
        print("üìä JSON: \(Int(jsonStartupTime))ms startup, \(Int(evaluationsPerSecondJSON)) evals/sec")
        print("üìä Protobuf Lazy: \(Int(protobufStartupTime))ms startup, \(Int(evaluationsPerSecondProtobuf)) evals/sec")
        print("\nüèÅ SPEEDUP:")
        print("   ‚ö° Startup: \(String(format: "%.1f", startupSpeedup))x faster with Protobuf")
        print("   üöÄ Evaluation: \(String(format: "%.1f", evaluationsPerSecondProtobuf/evaluationsPerSecondJSON))x protobuf vs json")

        // Cleanup
        jsonClient = nil
        protobufClient = nil
        configuration = nil

        print("\n‚úÖ JSON vs Protobuf Lazy performance benchmark completed!")

//        // Efficient JSON vs IkigaJSON Lazy performance comparison
//        // Focus: Startup performance with minimal evaluation testing
//
//        print("üöÄ Starting Efficient JSON vs IkigaJSON Lazy Performance Benchmark")
//        print("üéØ Focus: Fast startup comparison with minimal memory usage")
//
//        // Store only the aggregated results, not the full data
//        let jsonResults: PerformanceResults = try autoreleasepool {
//            print("\nüì¶ Benchmarking JSON Mode...")
//            let jsonData = try loadTestDataFile("flags-10000.json") // Use large dataset (~12MB)
//            print("   üìÑ JSON: \(ByteCountFormatter.string(fromByteCount: Int64(jsonData.count), countStyle: .binary))")
//
//            let results = try benchmarkJSONModeEfficient(jsonData: jsonData)
//
//            print("   üßπ JSON test completed, releasing memory...")
//            return results
//        } // JSON data automatically released here
//
//        // Force memory cleanup between modes
//        autoreleasepool {}
//
//        let ikigaResults: PerformanceResults = try autoreleasepool {
//            print("\nüì¶ Benchmarking IkigaJSON Lazy Mode...")
//            let jsonData = try loadTestDataFile("flags-10000.json") // Same large dataset (~12MB)
//            print("   üß†‚ö° IkigaJSON: \(ByteCountFormatter.string(fromByteCount: Int64(jsonData.count), countStyle: .binary))")
//
//            let results = try benchmarkIkigaJSONModeEfficient(jsonData: jsonData)
//
//            print("   üßπ IkigaJSON test completed, releasing memory...")
//            return results
//        } // IkigaJSON data automatically released here
//
//        // RESULTS COMPARISON (only store aggregations)
//        let startupSpeedup = jsonResults.startupTime / ikigaResults.startupTime
//        let evaluationSpeedup = ikigaResults.evaluationsPerSecond / jsonResults.evaluationsPerSecond
//
//        print("\nüèÜ PERFORMANCE RESULTS:")
//        print("üìä JSON: \(Int(jsonResults.startupTime))ms startup, \(Int(jsonResults.evaluationsPerSecond)) evals/sec")
//        print("üìä IkigaJSON: \(Int(ikigaResults.startupTime))ms startup, \(Int(ikigaResults.evaluationsPerSecond)) evals/sec")
//        print("\nüèÅ SPEEDUP:")
//        print("   ‚ö° Startup: \(String(format: "%.1f", startupSpeedup))x faster with IkigaJSON")
//        print("   üöÄ Evaluation: \(String(format: "%.1f", evaluationSpeedup))x relative performance")
//
//        // Simple assertions
//        XCTAssertLessThan(jsonResults.startupTime, 5000, "JSON startup should be under 5 seconds")
//        XCTAssertGreaterThan(ikigaResults.evaluationsPerSecond, 50, "IkigaJSON should handle at least 50 evals/sec")
//        XCTAssertGreaterThan(startupSpeedup, 1.0, "IkigaJSON should have faster startup")
//
//        print("\n‚úÖ Efficient performance benchmark completed!")
//    }
//
//    func testThreeWayPerformanceBenchmark() throws {
//        // This test benchmarks all three approaches: JSON, Direct FlatBuffer, and Lazy FlatBuffer
//        // to compare startup and evaluation performance across all modes
//
//        print("üöÄ Starting Three-Way Performance Benchmark")
//        print("üéØ Comparing: JSON, Direct FlatBuffer, and Lazy FlatBuffer modes")
//        print("üìä Key Metrics: Startup time (primary) and evaluation speed (secondary)")
//
//        // RUN JSON MODE FIRST (isolated)
//        let jsonResults = try autoreleasepool {
//            print("\nüì¶ Loading JSON data...")
//            let jsonData = try loadTestDataFile("flags-10000.json")
//            let testCases = try loadAllGeneratedTestCases()
//            print("   üìÑ JSON: \(ByteCountFormatter.string(fromByteCount: Int64(jsonData.count), countStyle: .binary))")
//
//            let results = try benchmarkJSONMode(jsonData: jsonData, testCases: testCases)
//
//            print("   üßπ Releasing JSON memory...")
//            return results
//        }
//
//        autoreleasepool {} // Force memory cleanup
//
//        // RUN DIRECT FLATBUFFER MODE SECOND (isolated)
////        let directFlatBufferResults = try autoreleasepool {
////            print("\nüì¶ Loading FlatBuffer data for direct mode...")
////            let flatBufferData = try loadTestDataFile("flags-10000.flatbuf")
////            let testCases = try loadAllGeneratedTestCases()
////            print("   ‚ö° FlatBuffer: \(ByteCountFormatter.string(fromByteCount: Int64(flatBufferData.count), countStyle: .binary))")
////
////            let results = try benchmarkFlatBufferMode(flatBufferData: flatBufferData, testCases: testCases)
////
////            print("   üßπ Releasing Direct FlatBuffer memory...")
////            return results
////        }
//
//        autoreleasepool {} // Force memory cleanup
//
//        // RUN LAZY FLATBUFFER MODE THIRD (isolated)
//        let lazyFlatBufferResults = try autoreleasepool {
//            print("\nüì¶ Loading FlatBuffer data for lazy mode...")
//            let flatBufferData = try loadTestDataFile("flags-10000.flatbuf")
//            let testCases = try loadAllGeneratedTestCases()
//            print("   üß† Lazy FlatBuffer: \(ByteCountFormatter.string(fromByteCount: Int64(flatBufferData.count), countStyle: .binary))")
//
//            let results = try benchmarkLazyFlatBufferMode(flatBufferData: flatBufferData, testCases: testCases)
//
//            print("   üßπ Releasing Lazy FlatBuffer memory...")
//            return results
//        }
//
//        // THREE-WAY RESULTS SUMMARY
//        print("\nüèÜ THREE-WAY PERFORMANCE BENCHMARK RESULTS:")
//        print("üìä JSON Mode:")
//        print("   üéØ Startup (KEY METRIC): \(String(format: "%.0f", jsonResults.startupTime))ms")
//        print("   ‚ö° Evaluation Speed: \(String(format: "%.0f", jsonResults.evaluationsPerSecond)) evals/sec")
//        print("   üíæ Memory Usage: \(String(format: "%.0f", jsonResults.memoryUsage))MB")
//        print("   üìä Total Evaluations: \(jsonResults.totalEvaluations)")
//
////        print("üìä Direct FlatBuffer Mode:")
////        print("   üéØ Startup: \(String(format: "%.0f", directFlatBufferResults.startupTime))ms")
////        print("   ‚ö° Evaluation Speed: \(String(format: "%.0f", directFlatBufferResults.evaluationsPerSecond)) evals/sec")
////        print("   üíæ Memory Usage: \(String(format: "%.0f", directFlatBufferResults.memoryUsage))MB")
////        print("   üìä Total Evaluations: \(directFlatBufferResults.totalEvaluations)")
//
//        print("üìä Lazy FlatBuffer Mode:")
//        print("   üéØ Startup: \(String(format: "%.0f", lazyFlatBufferResults.startupTime))ms")
//        print("   ‚ö° Evaluation Speed: \(String(format: "%.0f", lazyFlatBufferResults.evaluationsPerSecond)) evals/sec")
//        print("   üíæ Memory Usage: \(String(format: "%.0f", lazyFlatBufferResults.memoryUsage))MB")
//        print("   üìä Total Evaluations: \(lazyFlatBufferResults.totalEvaluations)")
//
//        // PERFORMANCE COMPARISONS
////        let directStartupSpeedup = jsonResults.startupTime / directFlatBufferResults.startupTime
//        let lazyStartupSpeedup = jsonResults.startupTime / lazyFlatBufferResults.startupTime
////        let directEvaluationSpeedup = directFlatBufferResults.evaluationsPerSecond / jsonResults.evaluationsPerSecond
//        let lazyEvaluationSpeedup = lazyFlatBufferResults.evaluationsPerSecond / jsonResults.evaluationsPerSecond
//
//        print("\nüèÅ THREE-WAY PERFORMANCE COMPARISON:")
//        print("   üìà Startup Performance (vs JSON):")
////        print("      ‚ö° Direct FlatBuffer: \(String(format: "%.1f", directStartupSpeedup))x faster")
//        print("      üß† Lazy FlatBuffer: \(String(format: "%.1f", lazyStartupSpeedup))x faster")
//        print("   üìà Evaluation Performance (vs JSON):")
////        print("      ‚ö° Direct FlatBuffer: \(String(format: "%.1f", directEvaluationSpeedup))x faster")
//        print("      üß† Lazy FlatBuffer: \(String(format: "%.1f", lazyEvaluationSpeedup))x faster")
//
//        print("\nüéØ Three-Way Performance Benchmark completed!")
//        print("   üìù Summary: Lazy FlatBuffer combines fast startup with proven JSON evaluation logic")
//
//        // Performance assertions
////        XCTAssertGreaterThan(directStartupSpeedup, 1.0, "Direct FlatBuffer should be faster than JSON for startup")
//        XCTAssertGreaterThan(lazyStartupSpeedup, 1.0, "Lazy FlatBuffer should be faster than JSON for startup")
//        XCTAssertGreaterThan(lazyFlatBufferResults.evaluationsPerSecond, 100, "Lazy mode should handle at least 100 evaluations per second")
//
//        // Explicit cleanup
//        autoreleasepool {}
//    }
//
//    func testFourWayPerformanceBenchmark() throws {
//        // This test benchmarks all four approaches: JSON, Direct FlatBuffer, Lazy FlatBuffer, and IkigaJSON Lazy
//        // to compare startup and evaluation performance across all modes
//
//        print("üöÄ Starting Four-Way Performance Benchmark")
//        print("üéØ Comparing: JSON, Direct FlatBuffer, Lazy FlatBuffer, and IkigaJSON Lazy modes")
//        print("üìä Key Metrics: Startup time (primary) and evaluation speed (secondary)")
//
//        // RUN JSON MODE FIRST (isolated)
//        let jsonResults = try autoreleasepool {
//            print("\nüì¶ Loading JSON data...")
//            let jsonData = try loadTestDataFile("flags-10000.json")
//            let testCases = try loadAllGeneratedTestCases()
//            print("   üìÑ JSON: \(ByteCountFormatter.string(fromByteCount: Int64(jsonData.count), countStyle: .binary))")
//
//            let results = try benchmarkJSONMode(jsonData: jsonData, testCases: testCases)
//
//            print("   üßπ Releasing JSON memory...")
//            return results
//        }
//
//        autoreleasepool {} // Force memory cleanup
//
//        // RUN DIRECT FLATBUFFER MODE SECOND (isolated)
//        let directFlatBufferResults = try autoreleasepool {
//            print("\nüì¶ Loading FlatBuffer data for direct mode...")
//            let flatBufferData = try loadTestDataFile("flags-10000.flatbuf")
//            let testCases = try loadAllGeneratedTestCases()
//            print("   ‚ö° FlatBuffer: \(ByteCountFormatter.string(fromByteCount: Int64(flatBufferData.count), countStyle: .binary))")
//
//            let results = try benchmarkFlatBufferMode(flatBufferData: flatBufferData, testCases: testCases)
//
//            print("   üßπ Releasing Direct FlatBuffer memory...")
//            return results
//        }
//
//        autoreleasepool {} // Force memory cleanup
//
//        // RUN LAZY FLATBUFFER MODE THIRD (isolated)
//        let lazyFlatBufferResults = try autoreleasepool {
//            print("\nüì¶ Loading FlatBuffer data for lazy mode...")
//            let flatBufferData = try loadTestDataFile("flags-10000.flatbuf")
//            let testCases = try loadAllGeneratedTestCases()
//            print("   üß† Lazy FlatBuffer: \(ByteCountFormatter.string(fromByteCount: Int64(flatBufferData.count), countStyle: .binary))")
//
//            let results = try benchmarkLazyFlatBufferMode(flatBufferData: flatBufferData, testCases: testCases)
//
//            print("   üßπ Releasing Lazy FlatBuffer memory...")
//            return results
//        }
//
//        autoreleasepool {} // Force memory cleanup
//
//        // RUN IKIGA JSON LAZY MODE FOURTH (isolated)
//        let ikigaJSONLazyResults = try autoreleasepool {
//            print("\nüì¶ Loading JSON data for IkigaJSON Lazy mode...")
//            let jsonData = try loadTestDataFile("flags-10000.json")
//            let testCases = try loadAllGeneratedTestCases()
//            print("   üß†‚ö° IkigaJSON Lazy: \(ByteCountFormatter.string(fromByteCount: Int64(jsonData.count), countStyle: .binary))")
//
//            let results = try benchmarkIkigaJSONLazyMode(jsonData: jsonData, testCases: testCases)
//
//            print("   üßπ Releasing IkigaJSON Lazy memory...")
//            return results
//        }
//
//        // FOUR-WAY RESULTS SUMMARY
//        print("\nüèÜ FOUR-WAY PERFORMANCE BENCHMARK RESULTS:")
//        print("üìä JSON Mode:")
//        print("   üéØ Startup (KEY METRIC): \(String(format: "%.0f", jsonResults.startupTime))ms")
//        print("   ‚ö° Evaluation Speed: \(String(format: "%.0f", jsonResults.evaluationsPerSecond)) evals/sec")
//        print("   üíæ Memory Usage: \(String(format: "%.0f", jsonResults.memoryUsage))MB")
//        print("   üìä Total Evaluations: \(jsonResults.totalEvaluations)")
//
//        print("üìä Direct FlatBuffer Mode:")
//        print("   üéØ Startup: \(String(format: "%.0f", directFlatBufferResults.startupTime))ms")
//        print("   ‚ö° Evaluation Speed: \(String(format: "%.0f", directFlatBufferResults.evaluationsPerSecond)) evals/sec")
//        print("   üíæ Memory Usage: \(String(format: "%.0f", directFlatBufferResults.memoryUsage))MB")
//        print("   üìä Total Evaluations: \(directFlatBufferResults.totalEvaluations)")
//
//        print("üìä Lazy FlatBuffer Mode:")
//        print("   üéØ Startup: \(String(format: "%.0f", lazyFlatBufferResults.startupTime))ms")
//        print("   ‚ö° Evaluation Speed: \(String(format: "%.0f", lazyFlatBufferResults.evaluationsPerSecond)) evals/sec")
//        print("   üíæ Memory Usage: \(String(format: "%.0f", lazyFlatBufferResults.memoryUsage))MB")
//        print("   üìä Total Evaluations: \(lazyFlatBufferResults.totalEvaluations)")
//
//        print("üìä IkigaJSON Lazy Mode:")
//        print("   üéØ Startup: \(String(format: "%.0f", ikigaJSONLazyResults.startupTime))ms")
//        print("   ‚ö° Evaluation Speed: \(String(format: "%.0f", ikigaJSONLazyResults.evaluationsPerSecond)) evals/sec")
//        print("   üíæ Memory Usage: \(String(format: "%.0f", ikigaJSONLazyResults.memoryUsage))MB")
//        print("   üìä Total Evaluations: \(ikigaJSONLazyResults.totalEvaluations)")
//
//        // PERFORMANCE COMPARISONS
//        let directStartupSpeedup = jsonResults.startupTime / directFlatBufferResults.startupTime
//        let lazyStartupSpeedup = jsonResults.startupTime / lazyFlatBufferResults.startupTime
//        let ikigaStartupSpeedup = jsonResults.startupTime / ikigaJSONLazyResults.startupTime
//        let directEvaluationSpeedup = directFlatBufferResults.evaluationsPerSecond / jsonResults.evaluationsPerSecond
//        let lazyEvaluationSpeedup = lazyFlatBufferResults.evaluationsPerSecond / jsonResults.evaluationsPerSecond
//        let ikigaEvaluationSpeedup = ikigaJSONLazyResults.evaluationsPerSecond / jsonResults.evaluationsPerSecond
//
//        print("\nüèÅ FOUR-WAY PERFORMANCE COMPARISON:")
//        print("   üìà Startup Performance (vs JSON):")
//        print("      ‚ö° Direct FlatBuffer: \(String(format: "%.1f", directStartupSpeedup))x faster")
//        print("      üß† Lazy FlatBuffer: \(String(format: "%.1f", lazyStartupSpeedup))x faster")
//        print("      üß†‚ö° IkigaJSON Lazy: \(String(format: "%.1f", ikigaStartupSpeedup))x faster")
//        print("   üìà Evaluation Performance (vs JSON):")
//        print("      ‚ö° Direct FlatBuffer: \(String(format: "%.1f", directEvaluationSpeedup))x faster")
//        print("      üß† Lazy FlatBuffer: \(String(format: "%.1f", lazyEvaluationSpeedup))x faster")
//        print("      üß†‚ö° IkigaJSON Lazy: \(String(format: "%.1f", ikigaEvaluationSpeedup))x faster")
//
//        print("\nüéØ Four-Way Performance Benchmark completed!")
//        print("   üìù Summary: Comparing all approaches for optimal startup+evaluation balance")
//
//        // Performance assertions
//        XCTAssertGreaterThan(directStartupSpeedup, 1.0, "Direct FlatBuffer should be faster than JSON for startup")
//        XCTAssertGreaterThan(lazyStartupSpeedup, 1.0, "Lazy FlatBuffer should be faster than JSON for startup")
//        XCTAssertGreaterThan(ikigaStartupSpeedup, 1.0, "IkigaJSON Lazy should be faster than JSON for startup")
//        XCTAssertGreaterThan(ikigaJSONLazyResults.evaluationsPerSecond, 100, "IkigaJSON Lazy mode should handle at least 100 evaluations per second")
//
//        // Explicit cleanup
//        autoreleasepool {}
//    }
//
//    // MARK: - Benchmark Methods
//
//    private func benchmarkJSONMode(jsonData: Data, testCases: [PerformanceTestCase]) throws -> PerformanceResults {
//        print("\nüîÑ Benchmarking JSON Mode...")
//
//        // CRITICAL MEASUREMENT: Startup Performance (JSON->objects conversion)
//        let memoryBefore = getCurrentMemoryUsage()
//        print("   üèÅ Starting JSON->objects conversion...")
//
//        let startupStart = CFAbsoluteTimeGetCurrent()
//        var configuration: Configuration? = try Configuration(flagsConfigurationJson: jsonData, obfuscated: false)
//        let startupTime = (CFAbsoluteTimeGetCurrent() - startupStart) * 1000
//
//        let memoryAfter = getCurrentMemoryUsage()
//        print("   ‚ö° Startup complete: \(String(format: "%.0f", startupTime))ms, Memory: +\(String(format: "%.0f", memoryAfter - memoryBefore))MB")
//
//        // Create assignment logger for realistic benchmarking
//        let assignmentLogger: EppoClient.AssignmentLogger = { assignment in
//            // Simulate realistic logging work (minimal processing)
//            _ = assignment.featureFlag.count + assignment.subject.count
//        }
//
//        // Create client for evaluations
//        var client: EppoClient? = EppoClient.initializeOffline(
//            sdkKey: "json-benchmark-key",
//            assignmentLogger: assignmentLogger,
//            initialConfiguration: configuration!
//        )
//
//        // SECONDARY MEASUREMENT: Evaluation Performance across ALL 2000 flags
//        print("   üèÉ Running evaluation performance benchmark across all flags...")
//        let evaluationStart = CFAbsoluteTimeGetCurrent()
//        var totalEvaluations = 0
//
//        // Standard test subjects for consistent evaluation
//        let standardSubjects = [
//            ("user_basic", [:]),
//            ("user_us", ["country": EppoValue(value: "US")]),
//            ("user_uk", ["country": EppoValue(value: "UK")]),
//            ("user_premium", ["tier": EppoValue(value: "premium"), "country": EppoValue(value: "US")]),
//            ("user_enterprise", ["tier": EppoValue(value: "enterprise"), "plan": EppoValue(value: "annual")])
//        ]
//
//        // Get all flag keys from configuration
//        let flagsConfiguration = configuration!.flagsConfiguration
//        let allFlagKeys = Array(flagsConfiguration.flags.keys)
//
//        print("   üìä Testing \(allFlagKeys.count) flags with \(standardSubjects.count) subjects each...")
//
//        // Evaluate every flag with every standard subject
//        for flagKey in allFlagKeys {
//            guard let flag = flagsConfiguration.flags[flagKey] else { continue }
//
//            for (subjectKey, subjectAttributes) in standardSubjects {
//                // Determine appropriate default based on flag's variation type
//                switch flag.variationType {
//                case .string:
//                    _ = client!.getStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: "default"
//                    )
//                case .numeric:
//                    _ = client!.getNumericAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: 0.0
//                    )
//                case .integer:
//                    _ = client!.getIntegerAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: 0
//                    )
//                case .boolean:
//                    _ = client!.getBooleanAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: false
//                    )
//                case .json:
//                    _ = client!.getJSONStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: "{}"
//                    )
//                }
//
//                totalEvaluations += 1
//            }
//        }
//
//        let evaluationTime = (CFAbsoluteTimeGetCurrent() - evaluationStart) * 1000
//        let evaluationsPerSecond = Double(totalEvaluations) / (evaluationTime / 1000.0)
//
//        print("   ‚úÖ Evaluations complete: \(totalEvaluations) in \(String(format: "%.0f", evaluationTime))ms")
//        print("   üìà Performance: \(String(format: "%.0f", evaluationsPerSecond)) evals/sec")
//
//        let results = PerformanceResults(
//            startupTime: startupTime,
//            evaluationTime: evaluationTime,
//            totalEvaluations: totalEvaluations,
//            evaluationsPerSecond: evaluationsPerSecond,
//            memoryUsage: memoryAfter
//        )
//
//        // Explicit cleanup for CI memory management
//        configuration = nil
//        client = nil
//
//        return results
//    }
//
//    private func benchmarkFlatBufferMode(flatBufferData: Data, testCases: [PerformanceTestCase]) throws -> PerformanceResults {
//        print("\nüîÑ Benchmarking FlatBuffer Mode...")
//
//        // CRITICAL MEASUREMENT: Startup Performance (no conversion, direct FlatBuffer access)
//        let memoryBefore = getCurrentMemoryUsage()
//        print("   üèÅ Starting direct FlatBuffer client creation...")
//
//        // Create assignment logger for realistic benchmarking (same as JSON mode)
//        let assignmentLogger: EppoClient.AssignmentLogger = { assignment in
//            // Simulate realistic logging work (minimal processing)
//            _ = assignment.featureFlag.count + assignment.subject.count
//        }
//
//        let startupStart = CFAbsoluteTimeGetCurrent()
//        var client: LazyFlatBufferClient? = try LazyFlatBufferClient(
//            sdkKey: "flatbuffer-benchmark-key",
//            flatBufferData: flatBufferData,
//            obfuscated: false,
//            assignmentLogger: assignmentLogger
//        )
//        let startupTime = (CFAbsoluteTimeGetCurrent() - startupStart) * 1000
//
//        let memoryAfter = getCurrentMemoryUsage()
//        print("   ‚ö° Startup complete: \(String(format: "%.0f", startupTime))ms, Memory: +\(String(format: "%.0f", memoryAfter - memoryBefore))MB")
//
//        // SECONDARY MEASUREMENT: Evaluation Performance across ALL 10K flags
//        print("   üèÉ Running evaluation performance benchmark across all flags...")
//        let evaluationStart = CFAbsoluteTimeGetCurrent()
//        var totalEvaluations = 0
//
//        // Standard test subjects for consistent evaluation
//        let standardSubjects = [
//            ("user_basic", [:]),
//            ("user_us", ["country": EppoValue(value: "US")]),
//            ("user_uk", ["country": EppoValue(value: "UK")]),
//            ("user_premium", ["tier": EppoValue(value: "premium"), "country": EppoValue(value: "US")]),
//            ("user_enterprise", ["tier": EppoValue(value: "enterprise"), "plan": EppoValue(value: "annual")])
//        ]
//
//        // Get all flag keys from FlatBuffer client
//        let allFlagKeys = client!.getAllFlagKeys()
//
//        print("   üìä Testing \(allFlagKeys.count) flags with \(standardSubjects.count) subjects each...")
//
//        // Evaluate every flag with every standard subject
//        for flagKey in allFlagKeys {
//            guard let flagVariationType = client!.getFlagVariationType(flagKey: flagKey) else { continue }
//
//            for (subjectKey, subjectAttributes) in standardSubjects {
//                // Determine appropriate default based on flag's variation type
//                switch flagVariationType {
//                case .string:
//                    _ = client!.getStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: "default"
//                    )
//                case .numeric:
//                    _ = client!.getNumericAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: 0.0
//                    )
//                case .integer:
//                    _ = client!.getIntegerAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: 0
//                    )
//                case .boolean:
//                    _ = client!.getBooleanAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: false
//                    )
//                case .json:
//                    _ = client!.getJSONStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: "{}"
//                    )
//                }
//
//                totalEvaluations += 1
//            }
//        }
//
//        let evaluationTime = (CFAbsoluteTimeGetCurrent() - evaluationStart) * 1000
//        let evaluationsPerSecond = Double(totalEvaluations) / (evaluationTime / 1000.0)
//
//        print("   ‚úÖ Evaluations complete: \(totalEvaluations) in \(String(format: "%.0f", evaluationTime))ms")
//        print("   üìà Performance: \(String(format: "%.0f", evaluationsPerSecond)) evals/sec")
//
//        let results = PerformanceResults(
//            startupTime: startupTime,
//            evaluationTime: evaluationTime,
//            totalEvaluations: totalEvaluations,
//            evaluationsPerSecond: evaluationsPerSecond,
//            memoryUsage: memoryAfter
//        )
//
//        // Explicit cleanup for CI memory management
//        client = nil
//
//        return results
//    }
//
//    private func benchmarkLazyFlatBufferMode(flatBufferData: Data, testCases: [PerformanceTestCase]) throws -> PerformanceResults {
//        print("\nüîÑ Benchmarking Lazy FlatBuffer Mode...")
//
//        // CRITICAL MEASUREMENT: Startup Performance (almost instantaneous)
//        let memoryBefore = getCurrentMemoryUsage()
//        print("   üèÅ Starting lazy FlatBuffer client creation...")
//
//        let startupStart = CFAbsoluteTimeGetCurrent()
//        var lazyClient: LazyFlatBufferClient? = try LazyFlatBufferClient(
//            sdkKey: "lazy-flatbuffer-benchmark-key",
//            flatBufferData: flatBufferData,
//            obfuscated: false,
//            assignmentLogger: { assignment in
//                // Simulate realistic logging work (minimal processing)
//                _ = assignment.featureFlag.count + assignment.subject.count
//            }
//        )
//        let startupTime = (CFAbsoluteTimeGetCurrent() - startupStart) * 1000
//
//        let memoryAfter = getCurrentMemoryUsage()
//        print("   ‚ö° Startup complete: \(String(format: "%.0f", startupTime))ms, Memory: +\(String(format: "%.0f", memoryAfter - memoryBefore))MB")
//
//        // SECONDARY MEASUREMENT: Evaluation Performance
//        print("   üèÉ Running evaluation performance benchmark across all flags...")
//
//        let evaluationStart = CFAbsoluteTimeGetCurrent()
//        var totalEvaluations = 0
//
//        // Get flag keys and test 5 subjects per flag (same as other modes)
//        let flagKeys = lazyClient!.getAllFlagKeys()
//        let subjects = ["user_basic", "user_us", "user_uk", "user_premium", "user_enterprise"]
//
//        print("   üìä Testing \(flagKeys.count) flags with \(subjects.count) subjects each...")
//
//        for flagKey in flagKeys {
//            guard let flagType = lazyClient!.getFlagVariationType(flagKey: flagKey) else { continue }
//
//            for subject in subjects {
//                totalEvaluations += 1
//                let attributes: [String: EppoValue] = ["country": EppoValue.valueOf("US")]
//
//                // Perform evaluation based on flag type
//                switch flagType {
//                case .boolean:
//                    _ = lazyClient!.getBooleanAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subject,
//                        subjectAttributes: attributes,
//                        defaultValue: false
//                    )
//                case .string:
//                    _ = lazyClient!.getStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subject,
//                        subjectAttributes: attributes,
//                        defaultValue: "default"
//                    )
//                case .integer:
//                    _ = lazyClient!.getIntegerAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subject,
//                        subjectAttributes: attributes,
//                        defaultValue: 0
//                    )
//                case .numeric:
//                    _ = lazyClient!.getNumericAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subject,
//                        subjectAttributes: attributes,
//                        defaultValue: 0.0
//                    )
//                case .json:
//                    _ = lazyClient!.getJSONStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subject,
//                        subjectAttributes: attributes,
//                        defaultValue: "{}"
//                    )
//                }
//            }
//        }
//
//        let evaluationTime = (CFAbsoluteTimeGetCurrent() - evaluationStart) * 1000
//        let evaluationsPerSecond = Double(totalEvaluations) / (evaluationTime / 1000)
//
//        print("   ‚úÖ Evaluations complete: \(totalEvaluations) in \(String(format: "%.0f", evaluationTime))ms")
//        print("   üìà Performance: \(String(format: "%.0f", evaluationsPerSecond)) evals/sec")
//
//        // Clean up
//        lazyClient = nil
//
//        return PerformanceResults(
//            startupTime: startupTime,
//            evaluationTime: evaluationTime,
//            totalEvaluations: totalEvaluations,
//            evaluationsPerSecond: evaluationsPerSecond,
//            memoryUsage: memoryAfter
//        )
//    }
//
//    private func benchmarkIkigaJSONLazyMode(jsonData: Data, testCases: [PerformanceTestCase]) throws -> PerformanceResults {
//        print("\nüîÑ Benchmarking IkigaJSON Lazy Mode...")
//
//        // CRITICAL MEASUREMENT: Startup Performance (fast JSON parsing without conversion)
//        let memoryBefore = getCurrentMemoryUsage()
//        print("   üèÅ Starting IkigaJSON Lazy client creation...")
//
//        let startupStart = CFAbsoluteTimeGetCurrent()
//        var ikigaClient: IkigaJSONLazyClient? = try IkigaJSONLazyClient(
//            sdkKey: "ikiga-json-lazy-benchmark-key",
//            jsonData: jsonData,
//            obfuscated: false,
//            assignmentLogger: { assignment in
//                // Simulate realistic logging work (minimal processing)
//                _ = assignment.featureFlag.count + assignment.subject.count
//            }
//        )
//        let startupTime = (CFAbsoluteTimeGetCurrent() - startupStart) * 1000
//
//        let memoryAfter = getCurrentMemoryUsage()
//        print("   ‚ö° Startup complete: \(String(format: "%.0f", startupTime))ms, Memory: +\(String(format: "%.0f", memoryAfter - memoryBefore))MB")
//
//        // SECONDARY MEASUREMENT: Evaluation Performance
//        print("   üèÉ Running evaluation performance benchmark across all flags...")
//
//        let evaluationStart = CFAbsoluteTimeGetCurrent()
//        var totalEvaluations = 0
//
//        // Get flag keys and test with standard subjects (same as other modes)
//        let flagKeys = ikigaClient!.getAllFlagKeys()
//        let standardSubjects = [
//            ("user_basic", [:]),
//            ("user_us", ["country": EppoValue(value: "US")]),
//            ("user_uk", ["country": EppoValue(value: "UK")]),
//            ("user_premium", ["tier": EppoValue(value: "premium"), "country": EppoValue(value: "US")]),
//            ("user_enterprise", ["tier": EppoValue(value: "enterprise"), "plan": EppoValue(value: "annual")])
//        ]
//
//        print("   üìä Testing \(flagKeys.count) flags with \(standardSubjects.count) subjects each...")
//
//        // Evaluate every flag with every standard subject
//        for flagKey in flagKeys {
//            guard let flagVariationType = ikigaClient!.getFlagVariationType(flagKey: flagKey) else { continue }
//
//            for (subjectKey, subjectAttributes) in standardSubjects {
//                // Determine appropriate default based on flag's variation type
//                switch flagVariationType {
//                case .string:
//                    _ = ikigaClient!.getStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: "default"
//                    )
//                case .numeric:
//                    _ = ikigaClient!.getNumericAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: 0.0
//                    )
//                case .integer:
//                    _ = ikigaClient!.getIntegerAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: 0
//                    )
//                case .boolean:
//                    _ = ikigaClient!.getBooleanAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: false
//                    )
//                case .json:
//                    _ = ikigaClient!.getJSONStringAssignment(
//                        flagKey: flagKey,
//                        subjectKey: subjectKey,
//                        subjectAttributes: subjectAttributes,
//                        defaultValue: "{}"
//                    )
//                }
//
//                totalEvaluations += 1
//            }
//        }
//
//        let evaluationTime = (CFAbsoluteTimeGetCurrent() - evaluationStart) * 1000
//        let evaluationsPerSecond = Double(totalEvaluations) / (evaluationTime / 1000.0)
//
//        print("   ‚úÖ Evaluations complete: \(totalEvaluations) in \(String(format: "%.0f", evaluationTime))ms")
//        print("   üìà Performance: \(String(format: "%.0f", evaluationsPerSecond)) evals/sec")
//
//        let results = PerformanceResults(
//            startupTime: startupTime,
//            evaluationTime: evaluationTime,
//            totalEvaluations: totalEvaluations,
//            evaluationsPerSecond: evaluationsPerSecond,
//            memoryUsage: memoryAfter
//        )
//
//        // Explicit cleanup for CI memory management
//        ikigaClient = nil
//
//        return results
//    }
//
    // MARK: - Efficient Benchmark Methods (Low Memory)

    private func benchmarkJSONModeEfficient(jsonData: Data) throws -> PerformanceResults {
        print("   üîÑ JSON startup test...")

        let memoryBefore = self.getCurrentMemoryUsage()
        let startupStart = CFAbsoluteTimeGetCurrent()

        // STARTUP MEASUREMENT
        var configuration: Configuration? = try Configuration(flagsConfigurationJson: jsonData, obfuscated: false)
        let startupTime = (CFAbsoluteTimeGetCurrent() - startupStart) * 1000

        let memoryAfter = self.getCurrentMemoryUsage()
        let memoryDiff = memoryAfter - memoryBefore
        print("   ‚ö° JSON startup: \(Int(startupTime))ms, Memory: +\(String(format: "%.1f", memoryDiff))MB")

        // MINIMAL EVALUATION TEST (only 100 evaluations instead of 50,000!)
        var client: EppoClient? = EppoClient.initializeOffline(
            sdkKey: "json-test",
            assignmentLogger: nil,
            initialConfiguration: configuration!
        )

        let evaluationStart = CFAbsoluteTimeGetCurrent()
        let flagKeys = Array(configuration!.flagsConfiguration.flags.keys.prefix(20)) // Only 20 flags
        var totalEvaluations = 0

        for flagKey in flagKeys {
            guard let flag = configuration!.flagsConfiguration.flags[flagKey] else { continue }
            for i in 0..<5 { // Only 5 subjects per flag = 100 total evaluations
                switch flag.variationType {
                case .boolean:
                    _ = client!.getBooleanAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: false)
                case .string:
                    _ = client!.getStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "")
                case .integer:
                    _ = client!.getIntegerAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0)
                case .numeric:
                    _ = client!.getNumericAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0.0)
                case .json:
                    _ = client!.getJSONStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "{}")
                }
                totalEvaluations += 1
            }
        }

        let evaluationTime = (CFAbsoluteTimeGetCurrent() - evaluationStart) * 1000
        let evaluationsPerSecond = Double(totalEvaluations) / (evaluationTime / 1000.0)

        print("   ‚úÖ JSON evals: \(totalEvaluations) in \(Int(evaluationTime))ms")

        // AGGRESSIVE CLEANUP
        client = nil
        configuration = nil

        return PerformanceResults(
            startupTime: startupTime,
            evaluationTime: evaluationTime,
            totalEvaluations: totalEvaluations,
            evaluationsPerSecond: evaluationsPerSecond,
            memoryUsage: memoryAfter
        )
    }

    private func benchmarkProtobufLazyModeEfficient(protobufData: Data) throws -> PerformanceResults {
        print("   üîÑ Protobuf startup test...")

        let memoryBefore = self.getCurrentMemoryUsage()
        let startupStart = CFAbsoluteTimeGetCurrent()

        // STARTUP MEASUREMENT
        var client: ProtobufLazyClient? = try ProtobufLazyClient(
            sdkKey: "protobuf-test",
            protobufData: protobufData,
            obfuscated: false,
            assignmentLogger: nil
        )
        let startupTime = (CFAbsoluteTimeGetCurrent() - startupStart) * 1000

        let memoryAfter = self.getCurrentMemoryUsage()
        let memoryDiff = memoryAfter - memoryBefore
        print("   ‚ö° Protobuf startup: \(Int(startupTime))ms, Memory: +\(String(format: "%.1f", memoryDiff))MB")

        // MINIMAL EVALUATION TEST (same 100 evaluations as JSON)
        let evaluationStart = CFAbsoluteTimeGetCurrent()
        let flagKeys = Array(client!.getAllFlagKeys().prefix(20)) // Same 20 flags
        var totalEvaluations = 0

        for flagKey in flagKeys {
            guard let flagType = client!.getFlagVariationType(flagKey: flagKey) else { continue }
            for i in 0..<5 { // Same 5 subjects = 100 evaluations
                switch flagType {
                case .boolean:
                    _ = client!.getBooleanAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: false)
                case .string:
                    _ = client!.getStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "")
                case .integer:
                    _ = client!.getIntegerAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0)
                case .numeric:
                    _ = client!.getNumericAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0.0)
                case .json:
                    _ = client!.getJSONStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "{}")
                }
                totalEvaluations += 1
            }
        }

        let evaluationTime = (CFAbsoluteTimeGetCurrent() - evaluationStart) * 1000
        let evaluationsPerSecond = Double(totalEvaluations) / (evaluationTime / 1000.0)

        print("   ‚úÖ Protobuf evals: \(totalEvaluations) in \(Int(evaluationTime))ms")

        // AGGRESSIVE CLEANUP
        client = nil

        return PerformanceResults(
            startupTime: startupTime,
            evaluationTime: evaluationTime,
            totalEvaluations: totalEvaluations,
            evaluationsPerSecond: evaluationsPerSecond,
            memoryUsage: memoryAfter
        )
    }

    private func benchmarkIkigaJSONModeEfficient(jsonData: Data) throws -> PerformanceResults {
        // TODO: Implement IkigaJSON benchmark when IkigaJSONLazyClient is available
        return PerformanceResults(
            startupTime: 0,
            evaluationTime: 0,
            totalEvaluations: 0,
            evaluationsPerSecond: 0,
            memoryUsage: 0
        )
//        print("   üîÑ IkigaJSON startup test...")
//
//        let memoryBefore = getCurrentMemoryUsage()
//        let startupStart = CFAbsoluteTimeGetCurrent()
//
//        // STARTUP MEASUREMENT
//        var client: IkigaJSONLazyClient? = try IkigaJSONLazyClient(
//            sdkKey: "ikiga-test",
//            jsonData: jsonData,
//            obfuscated: false,
//            assignmentLogger: nil
//        )
//        let startupTime = (CFAbsoluteTimeGetCurrent() - startupStart) * 1000
//
//        let memoryAfter = getCurrentMemoryUsage()
//        print("   ‚ö° IkigaJSON startup: \(Int(startupTime))ms, Memory: +\(Int(memoryAfter - memoryBefore))MB")
//
//        // MINIMAL EVALUATION TEST (same 100 evaluations as JSON)
//        let evaluationStart = CFAbsoluteTimeGetCurrent()
//        let flagKeys = Array(client!.getAllFlagKeys().prefix(20)) // Same 20 flags
//        var totalEvaluations = 0
//
//        for flagKey in flagKeys {
//            guard let flagType = client!.getFlagVariationType(flagKey: flagKey) else { continue }
//            for i in 0..<5 { // Same 5 subjects = 100 evaluations
//                switch flagType {
//                case .boolean:
//                    _ = client!.getBooleanAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: false)
//                case .string:
//                    _ = client!.getStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "")
//                case .integer:
//                    _ = client!.getIntegerAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0)
//                case .numeric:
//                    _ = client!.getNumericAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: 0.0)
//                case .json:
//                    _ = client!.getJSONStringAssignment(flagKey: flagKey, subjectKey: "test\(i)", subjectAttributes: [:], defaultValue: "{}")
//                }
//                totalEvaluations += 1
//            }
//        }
//
//        let evaluationTime = (CFAbsoluteTimeGetCurrent() - evaluationStart) * 1000
//        let evaluationsPerSecond = Double(totalEvaluations) / (evaluationTime / 1000.0)
//
//        print("   ‚úÖ IkigaJSON evals: \(totalEvaluations) in \(Int(evaluationTime))ms")
//
//        // AGGRESSIVE CLEANUP
//        client = nil
//
//        return PerformanceResults(
//            startupTime: startupTime,
//            evaluationTime: evaluationTime,
//            totalEvaluations: totalEvaluations,
//            evaluationsPerSecond: evaluationsPerSecond,
//            memoryUsage: memoryAfter
//        )
//    }
//
    // MARK: - Helper Methods

    private func loadTestDataFile(_ filename: String) throws -> Data {
        guard let fileURL = Bundle.module.url(
            forResource: "Resources/test-data/ufc/\(filename)",
            withExtension: ""
        ) else {
            throw TestError.fileNotFound("Could not find test data file: \(filename)")
        }

        return try Data(contentsOf: fileURL)
    }
//
//    private func loadAllGeneratedTestCases() throws -> [PerformanceTestCase] {
//        guard let testDataURL = Bundle.module.url(
//            forResource: "Resources/test-data/ufc/generated-tests",
//            withExtension: ""
//        ) else {
//            throw TestError.fileNotFound("Could not find generated-tests directory")
//        }
//
//        let fileManager = FileManager.default
//        let files = try fileManager.contentsOfDirectory(at: testDataURL, includingPropertiesForKeys: nil)
//
//        var testCases: [PerformanceTestCase] = []
//        for fileURL in files where fileURL.pathExtension == "json" {
//            let data = try Data(contentsOf: fileURL)
//            let testCase = try JSONDecoder().decode(PerformanceTestCase.self, from: data)
//            testCases.append(testCase)
//        }
//
//        return testCases
//    }
//
//    private func convertSubjectAttributes(_ attributes: [String: AnyCodable]) -> [String: EppoValue] {
//        return attributes.mapValues { anyValue in
//            switch anyValue.value {
//            case let stringValue as String:
//                return EppoValue(value: stringValue)
//            case let doubleValue as Double:
//                return EppoValue(value: doubleValue)
//            case let intValue as Int:
//                return EppoValue(value: intValue)
//            case let boolValue as Bool:
//                return EppoValue(value: boolValue)
//            default:
//                return EppoValue(value: "")
//            }
//        }
//    }
//
    private func getCurrentMemoryUsage() -> Double {
        var info = mach_task_basic_info()
        var count = mach_msg_type_number_t(MemoryLayout<mach_task_basic_info>.size)/4

        let kerr: kern_return_t = withUnsafeMutablePointer(to: &info) {
            $0.withMemoryRebound(to: integer_t.self, capacity: 1) {
                task_info(mach_task_self_,
                         task_flavor_t(MACH_TASK_BASIC_INFO),
                         $0,
                         &count)
            }
        }

        if kerr == KERN_SUCCESS {
            return Double(info.resident_size) / 1024.0 / 1024.0
        }

        return 0
    }
}

// MARK: - Data Models (Following EppoClientUFCTests Pattern)

enum TestError: Error {
    case fileNotFound(String)
}

// MARK: - Performance Results Tracking

struct PerformanceResults {
    let startupTime: Double          // Key metric - JSON->objects conversion time (ms)
    let evaluationTime: Double       // Secondary metric - total evaluation time (ms)
    let totalEvaluations: Int        // Number of evaluations performed
    let evaluationsPerSecond: Double // Evaluation throughput
    let memoryUsage: Double          // Memory usage (MB)
}
